{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "test_clustering_file = 'clustering_table.tsv'\n",
    "test_blacklist_samples_file = 'blacklist_samples.txt'\n",
    "\n",
    "test_clustering_df = pd.DataFrame(columns=['SAMPLE_NAME', 'CLUSTER_1', 'CLUSTER_2', 'CLUSTER_3'])\n",
    "test_num_clusters = 3\n",
    "test_num_samples = 10000\n",
    "test_sample_names = np.array(['sample_{:d}'.format(i) for i in range(1, test_num_samples + 1)])\n",
    "test_unnorm_responsibilities = np.random.random(size=(test_num_samples, test_num_clusters))\n",
    "test_responsibilities = test_unnorm_responsibilities / np.sum(test_unnorm_responsibilities, axis=1)[:, np.newaxis]\n",
    "test_clustering_df['SAMPLE_NAME'] = test_sample_names\n",
    "test_clustering_df[['CLUSTER_1', 'CLUSTER_2', 'CLUSTER_3']] = test_responsibilities\n",
    "test_clustering_df.to_csv(test_clustering_file, sep='\\t', index=False)\n",
    "\n",
    "test_blacklist_fraction = 0.05\n",
    "test_blacklist_samples = test_sample_names[1 == np.random.choice([0, 1], \n",
    "                                                                 p=[1 - test_blacklist_fraction, test_blacklist_fraction], \n",
    "                                                                 size=test_num_samples)]\n",
    "np.savetxt(test_blacklist_samples_file, test_blacklist_samples, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_cohorts_and_cases_and_write_results(output_dir: str,\n",
    "                                                  entity_ids: List[str],\n",
    "                                                  read_count_files: List[str],\n",
    "                                                  clustering_table_file: str, \n",
    "                                                  training_blacklist_file: str,\n",
    "                                                  number_of_training_samples_per_model: int,\n",
    "                                                  minimum_number_of_samples_per_cluster: int)\n",
    "    \n",
    "    #load clustering table\n",
    "    \n",
    "    #load training blacklist\n",
    "    \n",
    "    #determine MAP cluster identity for each sample\n",
    "    \n",
    "    #determine clusters below\n",
    "    \n",
    "    #make table of entity_id, read_count_file, training_blacklisted, min_size_blacklisted, grouped by cluster_id\n",
    "    \n",
    "    #for each cluster id, randomly select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--output_dir', \n",
    "                        type=str,    \n",
    "                        help='Output directory')\n",
    "    \n",
    "    parser.add_argument('--entity_ids', \n",
    "                        type=str,\n",
    "                        nargs='+',\n",
    "                        help='Entity IDs corresponding to read count files')\n",
    "    \n",
    "    parser.add_argument('--read_count_files', \n",
    "                        type=str,\n",
    "                        nargs='+',\n",
    "                        help='Read count files (output of GATK CollectReadCounts) corresponding to entity IDs')\n",
    "\n",
    "    parser.add_argument('--clustering_table_file',\n",
    "                        type=str,\n",
    "                        help='Table of clustered samples')\n",
    "    \n",
    "    parser.add_argument('--training_blacklist_file',\n",
    "                        type=str,\n",
    "                        help='Blacklist of samples not to be used for training models')\n",
    "    \n",
    "    parser.add_argument('--number_of_training_samples_per_model',\n",
    "                        type=int,\n",
    "                        help='Number of samples used to train each model')\n",
    "    \n",
    "    parser.add_argument('--minimum_number_of_samples_per_cluster',\n",
    "                        type=int,\n",
    "                        help='Samples in clusters that contain less than this number of samples will be output to a rescue list')\n",
    "\n",
    "    args = parser.parse_args()\n",
    " \n",
    "    output_dir = args.output_dir\n",
    "    entity_ids = args.entity_ids\n",
    "    read_count_files = args.read_count_files\n",
    "    clustering_table_file = args.clustering_table_file\n",
    "    training_blacklist_file = args.training_blacklist_file\n",
    "    number_of_training_samples_per_model = args.number_of_training_samples_per_model\n",
    "    minimum_number_of_samples_per_cluster = args.minimum_number_of_samples_per_cluster\n",
    "    \n",
    "    assert len(entity_ids) == len(read_count_files), \n",
    "        \"Number of entity IDs and number of read count files must be equal.\"\n",
    "    assert number_of_training_samples_per_model > 0, \"Number of training samples per model must be positive.\"\n",
    "    assert minimum_cluster_size >= number_of_training_samples_per_model, \n",
    "        \"Minimum number of samples per cluster must be greater than or equal to number of training samples per model.\"\n",
    "    \n",
    "\n",
    "    determine_cohorts_and_cases_and_write_results(output_dir,\n",
    "                                                  entity_ids,\n",
    "                                                  read_count_files,\n",
    "                                                  clustering_table_file, \n",
    "                                                  training_blacklist_file,\n",
    "                                                  number_of_training_samples_per_model,\n",
    "                                                  minimum_number_of_samples_per_cluster)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
