{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "test_entity_ids_file = 'entity_ids.txt'\n",
    "test_read_count_paths_file = 'read_count_paths.txt'\n",
    "test_clustering_file = 'clustering_table.tsv'\n",
    "test_blacklist_samples_file = 'blacklist_samples.txt'\n",
    "\n",
    "test_num_samples = 10000\n",
    "test_entity_ids = ['sample_{:d}'.format(i) \n",
    "                   for i in range(1, test_num_samples + 1)]\n",
    "np.savetxt(test_entity_ids_file, test_entity_ids, fmt='%s')\n",
    "\n",
    "test_read_count_paths = ['gs://fake-bucket/sample_{:d}.counts.tsv'.format(i) \n",
    "                         for i in range(1, test_num_samples + 1)]\n",
    "np.savetxt(test_read_count_paths_file, test_read_count_paths, fmt='%s')\n",
    "\n",
    "test_num_clusters = 25\n",
    "test_cluster_names = ['CLUSTER_{:d}'.format(i) \n",
    "                      for i in range(1, test_num_clusters + 1)]\n",
    "test_clustering_df = pd.DataFrame(columns=['SAMPLE_NAME'] + test_cluster_names)\n",
    "test_unnorm_responsibilities = np.random.random(size=(test_num_samples, test_num_clusters))\n",
    "test_responsibilities = test_unnorm_responsibilities / np.sum(test_unnorm_responsibilities, axis=1)[:, np.newaxis]\n",
    "test_clustering_df['SAMPLE_NAME'] = test_entity_ids\n",
    "test_clustering_df[test_cluster_names] = test_responsibilities\n",
    "test_clustering_df.to_csv(test_clustering_file, sep='\\t', index=False)\n",
    "\n",
    "test_blacklist_fraction = 0.05\n",
    "test_blacklist_samples = test_sample_names[1 == np.random.choice([0, 1], \n",
    "                                                                 p=[1 - test_blacklist_fraction, test_blacklist_fraction], \n",
    "                                                                 size=test_num_samples)]\n",
    "np.savetxt(test_blacklist_samples_file, test_blacklist_samples, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_cohorts_and_cases_and_write_results(output_dir: str,\n",
    "                                                  entity_ids_file: List[str],\n",
    "                                                  read_count_paths_file: List[str],\n",
    "                                                  clustering_table_file: str, \n",
    "                                                  training_blacklist_file: str,\n",
    "                                                  number_of_training_samples_per_model: int,\n",
    "                                                  minimum_number_of_samples_per_cluster: int):\n",
    "    #load entity ids\n",
    "    \n",
    "    #load read count paths\n",
    "    assert len(entity_ids) == len(read_count_files), \\\n",
    "        \"Number of entity IDs and number of read count files must be equal.\"\n",
    "    \n",
    "    #load clustering table\n",
    "    clustering_table_df = pd.DataFrame(clustering_table_file, sep='\\t')\n",
    "    cluster_names = clustering_table_df.columns[1:]\n",
    "    \n",
    "    #load training blacklist\n",
    "    training_blacklist = np.loadtxt(training_blacklist_file, dtype=str)\n",
    "    \n",
    "    #determine MAP cluster identity for each sample\n",
    "    \n",
    "    #determine clusters below minimum_number_of_samples_per_cluster\n",
    "    \n",
    "    #make table of entity_id, read_count_file, training_blacklisted, min_size_blacklisted, grouped by cluster_id\n",
    "    \n",
    "    #for each cluster id, randomly select training/case samples and output to lists\n",
    "    \n",
    "    #output complete table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '.'\n",
    "number_of_training_samples_per_model = 200\n",
    "minimum_number_of_samples_per_cluster = 100\n",
    "\n",
    "determine_cohorts_and_cases_and_write_results(output_dir: str,\n",
    "                                                  entity_ids_file: List[str],\n",
    "                                                  read_count_paths_file: List[str],\n",
    "                                                  clustering_table_file: str, \n",
    "                                                  training_blacklist_file: str,\n",
    "                                                  number_of_training_samples_per_model: int,\n",
    "                                                  minimum_number_of_samples_per_cluster: int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR]\n",
      "                             [--entity_ids ENTITY_IDS [ENTITY_IDS ...]]\n",
      "                             [--read_count_files READ_COUNT_FILES [READ_COUNT_FILES ...]]\n",
      "                             [--clustering_table_file CLUSTERING_TABLE_FILE]\n",
      "                             [--training_blacklist_file TRAINING_BLACKLIST_FILE]\n",
      "                             [--number_of_training_samples_per_model NUMBER_OF_TRAINING_SAMPLES_PER_MODEL]\n",
      "                             [--minimum_number_of_samples_per_cluster MINIMUM_NUMBER_OF_SAMPLES_PER_CLUSTER]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1890032050/jupyter/kernel-8383480e-4347-4106-809e-38fa9d8a48ad.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/BROAD.MIT.EDU/slee/.pyenv/versions/anaconda3-4.3.1/envs/gatk-ml/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--output_dir', \n",
    "                        type=str,    \n",
    "                        help='Output directory')\n",
    "    \n",
    "    parser.add_argument('--entity_ids_file', \n",
    "                        type=str,\n",
    "                        help='File containing entity IDs corresponding to read count paths')\n",
    "    \n",
    "    parser.add_argument('--read_count_paths_file', \n",
    "                        type=str,\n",
    "                        help='File containing read count paths (output of GATK CollectReadCounts) corresponding to entity IDs')\n",
    "\n",
    "    parser.add_argument('--clustering_table_file',\n",
    "                        type=str,\n",
    "                        help='Table of clustered samples (output of ClusterSamplesFromCoverage workflow)')\n",
    "    \n",
    "    parser.add_argument('--training_blacklist_file',\n",
    "                        type=str,\n",
    "                        help='File containing blacklist of entity IDs for samples not to be used for training models')\n",
    "    \n",
    "    parser.add_argument('--number_of_training_samples_per_model',\n",
    "                        type=int,\n",
    "                        help='Number of samples used to train each model')\n",
    "    \n",
    "    parser.add_argument('--minimum_number_of_samples_per_cluster',\n",
    "                        type=int,\n",
    "                        help='Minimum number of samples required in each cluster before building a model; ' + \n",
    "                             'entity IDs for samples in failing clusters will be output to a rescue list')\n",
    "\n",
    "    args = parser.parse_args()\n",
    " \n",
    "    output_dir = args.output_dir\n",
    "    entity_ids_file = args.entity_ids_file\n",
    "    read_count_paths_file = args.read_count_paths_file\n",
    "    clustering_table_file = args.clustering_table_file\n",
    "    training_blacklist_file = args.training_blacklist_file\n",
    "    number_of_training_samples_per_model = args.number_of_training_samples_per_model\n",
    "    minimum_number_of_samples_per_cluster = args.minimum_number_of_samples_per_cluster\n",
    "    \n",
    "    assert number_of_training_samples_per_model > 0, \"Number of training samples per model must be positive.\"\n",
    "    assert minimum_cluster_size >= number_of_training_samples_per_model, \\\n",
    "        \"Minimum number of samples per cluster must be greater than or equal to number of training samples per model.\"\n",
    "    \n",
    "\n",
    "    determine_cohorts_and_cases_and_write_results(output_dir,\n",
    "                                                  entity_ids_file,\n",
    "                                                  read_count_paths_file,\n",
    "                                                  clustering_table_file, \n",
    "                                                  training_blacklist_file,\n",
    "                                                  number_of_training_samples_per_model,\n",
    "                                                  minimum_number_of_samples_per_cluster)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
